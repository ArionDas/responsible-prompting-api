{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b95ba48",
   "metadata": {},
   "source": [
    "# Responsible Prompting\n",
    "\n",
    "## Recipe: Populate embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f3b42-7d2b-4914-ac48-e01132744279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c5498911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9210e4-0537-459f-be12-7381da11d338",
   "metadata": {},
   "source": [
    "### Loading hugging face token from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "45b95c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "HF_URL = os.getenv('HF_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b87a3c65-0e08-4fa9-aa8f-2f9a2f6c3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF URL: https://api-inference.huggingface.co/pipeline/feature-extraction/\n"
     ]
    }
   ],
   "source": [
    "# Double checking that the content loaded correctly\n",
    "print(f\"HF URL: {HF_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7cb62-3825-4ca9-be99-c94c2cf34127",
   "metadata": {},
   "source": [
    "### Sentence transformer model ids (from hugging face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "95fb523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These codes will be used in the hugging face request headers.\n",
    "# If you want to add more models, this is the place\n",
    "model_ids = [\n",
    "    # \"sentence-transformers/all-MiniLM-L6-v2\", \n",
    "    # \"ibm-granite/granite-embedding-30m-english\", \n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"intfloat/multilingual-e5-large\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11d170",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "cd09f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts model_id into filenames\n",
    "def model_id_to_filename( model_id ):\n",
    "    return model_id.split('/')[1].lower()\n",
    "\n",
    "# Requests embeddings for a given sentence\n",
    "def query( texts, model_id ):    \n",
    "    # Warning in case of prompts longer than 256 words\n",
    "    for t in texts :\n",
    "        n_words = len( re.split(r\"\\s+\", t ) )\n",
    "        if( n_words > 256 and model_id == \"sentence-transformers/all-MiniLM-L6-v2\" ):\n",
    "            warnings.warn( \"Warning: Sentence provided is longer than 256 words. Model all-MiniLM-L6-v2 expects sentences up to 256 words.\" )    \n",
    "            warnings.warn( \"Word count: {}\".format( n_words ) ) \n",
    "\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "    print( \"Request url: \" + api_url )\n",
    "    response = requests.post(api_url, headers=headers, json={\"inputs\": texts })\n",
    "\n",
    "    # Debugging API HTTP responses\n",
    "    # print(f'Status Code: {response.status_code}')\n",
    "    # print('Headers:')\n",
    "    # for key, value in response.headers.items():\n",
    "    #     print(f'  {key}: {value}')\n",
    "    # print('Content:')\n",
    "    # print(response.text)\n",
    "    \n",
    "    out = response.json() \n",
    "    # making sure that different transformers retrieve the embedding\n",
    "    if( 'error' in out ):\n",
    "        return out\n",
    "    while( len( out ) < 384 ): # unpacking json responses in the form of [[[embedding]]]\n",
    "        out = out[0]\n",
    "    return out\n",
    "\n",
    "# Returns the centroid for a given value\n",
    "def get_centroid( v, dimension = 384, k = 10 ):\n",
    "    centroid = [0] * dimension\n",
    "    count = 0            \n",
    "    for p in v['prompts']:\n",
    "        i = 0\n",
    "        while i < len( p['embedding'] ):\n",
    "            centroid[i] += p['embedding'][i]\n",
    "            i += 1\n",
    "        count += 1\n",
    "    i = 0\n",
    "    while i < len( centroid ):\n",
    "        centroid[i] /= count\n",
    "        i += 1\n",
    "\n",
    "    return centroid    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39191c3",
   "metadata": {},
   "source": [
    "### Populating JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "87316fa4-1fcf-41c4-9913-bc5704b25ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening existing file:  ../prompt-sentences-main/prompt_sentences-bge-large-en-v1.5.json\n",
      "Request url: https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5\n",
      "Dimensions from hugging face API response: 1024\n",
      "Dimensions from json file: 1024\n",
      "Old prompts:  2194\n",
      "New prompts:  0\n",
      "Errors:  0\n",
      "Successes:  0\n",
      "Updating centroids.\n",
      "Saving into file:  ../prompt-sentences-main/prompt_sentences-bge-large-en-v1.5.json\n",
      "\n",
      "\n",
      "Opening existing file:  ../prompt-sentences-main/prompt_sentences-multilingual-e5-large.json\n",
      "Request url: https://api-inference.huggingface.co/models/intfloat/multilingual-e5-large\n",
      "Dimensions from hugging face API response: 1024\n",
      "Dimensions from json file: 1024\n",
      "Old prompts:  2194\n",
      "New prompts:  0\n",
      "Errors:  0\n",
      "Successes:  0\n",
      "Updating centroids.\n",
      "Saving into file:  ../prompt-sentences-main/prompt_sentences-multilingual-e5-large.json\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON folder\n",
    "json_folder = '../prompt-sentences-main/'\n",
    "\n",
    "# INPUT FILE\n",
    "# Default file with empty embeddings\n",
    "json_in_file = json_folder + 'prompt_sentences.json' \n",
    "\n",
    "# trying to open the input file first\n",
    "if( os.path.isfile( json_in_file ) ):    \n",
    "    prompt_json_in = json.load( open( json_in_file ) )\n",
    "\n",
    "for model_id in model_ids:\n",
    "    # OUTPUT FILE\n",
    "    json_out_file_suffix = model_id_to_filename( model_id )\n",
    "    json_out_file = f\"{json_folder}prompt_sentences-{json_out_file_suffix}.json\"\n",
    "\n",
    "    # Trying to open the files first\n",
    "    if( os.path.isfile( json_out_file ) ):    \n",
    "        prompt_json_out = json.load( open( json_out_file ) )\n",
    "        print( 'Opening existing file: ', json_out_file )\n",
    "\n",
    "        # API request test\n",
    "        api_response_dimensions = len( query( ['testing API endpoint'], model_id ) )\n",
    "        print( f\"Dimensions from hugging face API response: {api_response_dimensions}\" )\n",
    "        json_file_dimensions = len( prompt_json_out['positive_values'][0]['prompts'][0]['embedding'] )\n",
    "        print( f\"Dimensions from json file: {json_file_dimensions}\" )\n",
    "        if( api_response_dimensions != json_file_dimensions ):\n",
    "            warnings.warn( f\"Dimensions are different: API={api_response_dimensions} while JSON sentences file={json_file_dimensions}\" )     \n",
    "    else:\n",
    "        # Creating an empty file for new transformer\n",
    "        with open( json_out_file, 'w') as outfile: \n",
    "            json.dump( prompt_json_in, outfile)\n",
    "        prompt_json_out = json.load( open( json_out_file ) )\n",
    "        print( 'Creating a new file: ', json_out_file )\n",
    "\n",
    "    ############################\n",
    "    # Generate a new output file using the hashmap as auxiliary table hosting old and new/changed embeddings\n",
    "    ############################\n",
    "    \n",
    "    # Using the output json with the prompts and embeddings\n",
    "    # prompt_json_out\n",
    "    \n",
    "    # Create a hashmap with a key value containing a hash for the prompt and the already populated embedding\n",
    "    prompts_embeddings = {}\n",
    "    new_prompts = 0\n",
    "    old_prompts = 0\n",
    "    errors = 0\n",
    "    successes = 0\n",
    "    \n",
    "    for v in prompt_json_out['positive_values']:\n",
    "        for p in v['prompts']:\n",
    "            if( p['embedding'] != [] ):\n",
    "                prompts_embeddings[ p['text'] ] = p['embedding']\n",
    "    \n",
    "    for v in prompt_json_out['negative_values']:\n",
    "        for p in v['prompts']:\n",
    "            if( p['embedding'] != [] ):\n",
    "                prompts_embeddings[ p['text'] ] = p['embedding']\n",
    "            \n",
    "    # Loading all prompts from the input json, potentially with new/changed sentences\n",
    "    # prompt_json_in\n",
    "    \n",
    "    # Iterate over the two lists, looking only for new/changed prompts that require the API request for embeddings\n",
    "    for v in prompt_json_in['positive_values']:\n",
    "        for p in v['prompts']:\n",
    "            if( p['text'] in prompts_embeddings ):\n",
    "                # Prompt found, no need to request embeddings\n",
    "                p['embedding'] = prompts_embeddings[ p['text'] ]\n",
    "                old_prompts += 1\n",
    "            else:\n",
    "                # Requesting embedding for new/changed prompt\n",
    "                embedding = query( p['text'] )\n",
    "                if( 'error' in embedding ):\n",
    "                    errors += 1\n",
    "                else: \n",
    "                    # Add the new/changed prompt to the hashmap\n",
    "                    prompts_embeddings[ p['text'] ] = embedding\n",
    "                    \n",
    "                    # Using the new hash\n",
    "                    p['embedding'] = prompts_embeddings[ p['text'] ]\n",
    "                    successes += 1\n",
    "                new_prompts += 1\n",
    "            \n",
    "    for v in prompt_json_in['negative_values']:\n",
    "        for p in v['prompts']:\n",
    "            if( p['text'] in prompts_embeddings ):\n",
    "                # Prompt found, no need to request embeddings\n",
    "                p['embedding'] = prompts_embeddings[ p['text'] ]\n",
    "                old_prompts += 1\n",
    "            else:\n",
    "                # Requesting embedding for new/changed prompt\n",
    "                embedding = query( p['text'] )\n",
    "                if( 'error' in embedding ):\n",
    "                    errors += 1\n",
    "                else: \n",
    "                    # Add the new/changed prompt to the hashmap\n",
    "                    prompts_embeddings[ p['text'] ] = embedding\n",
    "                    \n",
    "                    # Using the new hash\n",
    "                    p['embedding'] = prompts_embeddings[ p['text'] ]\n",
    "                    successes += 1\n",
    "                new_prompts += 1\n",
    "    \n",
    "    print( 'Old prompts: ', old_prompts )\n",
    "    print( 'New prompts: ', new_prompts )\n",
    "    print( 'Errors: ', errors )\n",
    "    print( 'Successes: ', successes )\n",
    "\n",
    "    # After all the embeddings are populated (with no errors), compute the centroids for each value\n",
    "    if( errors == 0 ):\n",
    "        print( 'Updating centroids.' )\n",
    "        for v in prompt_json_in['positive_values']:\n",
    "            v['centroid'] = get_centroid( v, json_file_dimensions, 10 )\n",
    "        for v in prompt_json_in['negative_values']:\n",
    "            v['centroid'] = get_centroid( v, json_file_dimensions, 10 )\n",
    "\n",
    "    # Saving the embeddings for a specific LLM\n",
    "    with open( json_out_file, 'w') as outfile:\n",
    "        print( 'Saving into file: ', json_out_file )\n",
    "        json.dump( prompt_json_in, outfile)   \n",
    "        print( '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a257009-4021-4956-a3ee-5d39931ecd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5a03b-5ebf-4361-a183-4a19261e4ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
